---
author:
  name: Mihalis Tsoukalos
  email: mihalistsoukalos@gmail.com
description: 'Using AWK for processing Apache log files.'
keywords: ["UNIX", "shell", "AWK", "Apache", "logs"]
license: '[CC BY-ND 4.0](https://creativecommons.org/licenses/by-nd/4.0)'
published: 2019-06-27
modified_by:
  name: Linode
title: 'Learning to use AWK for processing Apache log files'
contributor:
  name: Mihalis Tsoukalos
  link: https://www.mtsoukalos.eu/
external_resources:
  - '[Apache Web Server](https://httpd.apache.org/)'
---

## Before You Begin

{{< note >}}
This guide is written for a non-root user. Depending on your configuration, some commands might require the help of `sudo` in order to get property executed. If you are not familiar with the `sudo` command, see the [Users and Groups](/docs/tools-reference/linux-users-and-groups/) guide.
{{< /note >}}

## Introduction

The purpose of this guide is to illustrate how to use the AWK programming language
for processing Apache log files.

Most of the presented examples are relatively small and therefore you will not need
to save the AWK code in an external file. However, you are free to do so on your own.

Additionally, most of the presented commands use traditional UNIX utilities such as
`grep(1)`, `head(1)`, `sort(1)`, `wc(1)` and `uniq(1)` to filter or process the data.

### An Introduction to Apache Log Files

{{< note >}}
This section will illustrate how Apache log files look like. However, some of the
presented information such as IP addresses and URLs might have been altered for
reasons of security.
{{< /note >}}

The Apache log file format that will be used in this guide will create log entries
similar to the following:

{{< output >}}
107.100.13.181 - - [24/Jun/2019:06:25:50 +0300] "GET /sites/default/files/LUD160.png HTTP/1.1" 200 810274 "-" "Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)" 1439502
46.229.200.247 - - [24/Jun/2019:07:00:37 +0300] "GET /robots.txt HTTP/1.1" 200 2925 "-" "Mozilla/5.0 (compatible; SemrushBot/3~bl; +http://www.semrush.com/bot.html)" 831
{{< /output >}}

{{< note >}}
If you are using a different Apache log file format, you will most likely need to make
small changes to the presented AWK commands in order to extract the desired fields.
{{< /note >}}

### Changing the format of Apache Log Files

If you wish to change the format of the log entries generated by Apache or inspect
the details of the format that is currently being used, you should make changes or
view the configuration file of your Apache Virtual web sites. On a Debian system,
these files can be found inside `/etc/apache2`.

If you execute the following command, you will find the Apache log format that is
currently being used by the `mtsoukalos.eu` domain:

    grep Log /etc/apache2/sites-available/mtsoukalos.eu
{{< output >}}
     ErrorLog /srv/www/www.mtsoukalos.eu/logs/error.log
     CustomLog /srv/www/www.mtsoukalos.eu/logs/access.log myformat
{{< /output >}}

This means that the log format that is being used is called `myformat`. In order to find
the definition of that log format, you will have to look inside `/etc/apache2/apache.conf`:

    grep myformat /etc/apache2/apache2.conf
{{< output >}}
LogFormat "%h %l %u %t \"%r\" %>s %O \"%{Referer}i\" \"%{User-Agent}i\" %D" myformat
{{< /output >}}

Each log format definition begins by the `LogFormat` keyword and is followed by the
fields that you want to include in the log records. The last word in the `LogFormat`
line is the name of the log format.

If you want to learn more about the meaning of each one of the format strings in `LogFormat`
entries, you should visit [this](http://httpd.apache.org/docs/current/mod/mod_log_config.html).

## Calculating the Total Number of Bytes Served

The following code calculates the total number of bytes that have been served:

    cat access.log | awk '{ sum += $10 } END { print sum }'
{{< output >}}
27394860
{{< /output >}}

The AWK code tells AWK to extract the 10th field, which is the bytes that have
been served on the request and add that number to the `sum` variable, which
is printed at the end.

Please notice that this command can be rewritten as

    awk '{ sum += $10 } END { print sum }' access.log

## Calculating the Total Number of Seconds connected

This AWK script is pretty similar to the previous one as it calculates the
total number of seconds clients were connected to our server using the
values stored in the 13th field of each log record:

    cat access.log | awk '{ sum += $NF } END { print sum/1000000 }'
{{< output >}}
85.0389
{{< /output >}}

As the last field in the log record is the time it took to serve the request and is in
microseconds, we have to divide the result by `1000000` to convert it into seconds.

## Printing the Number of Unique IP addresses

The following command finds the number of unique IP addresses stored on an Apache log file:

    cat access.log | awk '{print $1}' | sort | uniq | wc -l
{{< output >}}
279
{{< /output >}}

The `awk '{print $1}'` command prints the first column of each line, which is the IP
address of the client machine. The remaining commands sort the output, filter out
repeated lines in the output and count the number of lines the generated output has.

## Extracting specific lines

This technique uses a simple regular expression to extract the lines in the log
file that contain a specific string, which is this case is `robots.txt`:

    awk '/robots.txt/' access.log
{{< output >}}
46.229.168.147 - - [24/Jun/2019:07:00:37 +0300] "GET /robots.txt HTTP/1.1" 200 2925 "-" "Mozilla/5.0 (compatible; SemrushBot/3~bl; +http://www.semrush.com/bot.html)" 831
216.244.66.239 - - [24/Jun/2019:07:32:53 +0300] "GET /robots.txt HTTP/1.1" 200 4543 "-" "Mozilla/5.0 (compatible; DotBot/1.1; http://www.opensiteexplorer.org/dotbot, help@moz.com)" 692
{{< /output >}}

You are free to continue processing the generated output or search for something different.

## Finding the HTTP protocol used along with its Version

This example will illustrate how to extract HTTP protocol related information from
your Apache log files:

    awk '{print $8}' access.log | sort | uniq -c | sort -rn
{{< output >}}
1592 HTTP/1.1"
  61 HTTP/1.0"
{{< /output >}}

## Finding Information about Status Codes

This is a truly handy case as you will extract information about the status codes
of your requests:

    awk '{print $9}' access.log | sort | uniq -c | sort -rn
{{< output >}}
1596 200
  25 404
  18 403
   9 304
   5 302
{{< /output >}}

As the status code is stored in the 9th field of each log record, you will have to
extract it before processing the data using traditional UNIX tools.

## Creating a Histogram

This section presents a pretty clever use of AWK and its capabilities by printing a
histogram on the screen instead of numeric values.

The following AWK code prints a histogram of the most popular IP addresses that
were found in the current log file:

    cat access.log | awk '{print $1}' | uniq -c | sort -nr | awk '{printf("\n%s ",$0) ; for (i = 0; i<$1 ; i++) {printf("*")};}' | head
{{< output >}}
  44 51.15.191.178 ********************************************
  39 146.214.64.253 ***************************************
  38 94.69.18.243 **************************************
  38 46.246.199.10 **************************************
  38 173.75.229.4 **************************************
  37 83.165.69.246 *************************************
  37 208.117.252.6 *************************************
  34 177.8.80.142 **********************************
  32 2.84.157.144 ********************************
{{< /output >}}

The same technique can be used for presenting other kinds of information such as
HTTP status codes:

    cat access.log | awk '{print $9}' | sort | uniq -c | sort -nr | awk '{printf("\n%s ",$0) ; for (i = 0; i<$1 ; i++) {printf("*")};}' | head

## Extracting all unique user agents

You can extract the user agents that can be found in a log file and count the number
of the appearances of each user agent as follows:

    awk -F\" '($2 ~ "^GET /"){print $6}' ./access.log | sort | uniq -c
{{< output >}}
     4 -
     85 Barkrowler/0.9 (+http://www.exensa.com/crawl)
     13 Feedly/1.0 (+http://www.feedly.com/fetcher.html; 1 subscribers; like FeedFetcher-Google)
      2 Go-http-client/1.1
      1 Googlebot-Image/1.0
{{< /output >}}

If you want to sort the output based on the number of times each user agent was
found in the log files, you can execute the next command:

    awk -F\" '($2 ~ "^GET /"){print $6}' ./access.log | sort | uniq -c | sort -rn
{{< output >}}
    293 Mozilla/5.0 (compatible; SemrushBot/3~bl; +http://www.semrush.com/bot.html)
    181 Mozilla/5.0 (compatible; bingbot/2.0; +http://www.bing.com/bingbot.htm)
    104 Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/74.0.3729.169 Safari/537.36
{{< /output >}}

## Calculating the number of times an IP Accessed a URL

This technique calculates the number of times each IP accessed a given URL (`/`).

	awk -F'[ "]+' '$7 == "/" { ipcount[$1]++ }
	    END { for (i in ipcount) {
	        printf "%15s - %d\n", i, ipcount[i] } }' ./access.log
{{< output >}}
15.236.186.116 - 2
 16.214.6.23 - 2
101.20.154.14 - 1
{{< /output >}}

The `$7` field is the requested URL, which is this case is compared against `/`. However,
you can use any URL you like.

## Which URLs have been accessed by a Given IP 

This technique allows you to calculate how many times each URL has been accessed
by a given IP address.

	awk -F'[ "]+' '$1 == "146.27.13.182" { pagecount[$7]++ }
	    END { for (i in pagecount) {
	        printf "%15s - %d\n", i, pagecount[i] } }' access.log
{{< output >}}
/misc/jquery.js?v=1.4.4 - 1
/modules/system/system.theme.css?prby0d - 1
/misc/favicon.ico - 1
{{< /output >}}

## Calculating the times Resources have been Requested

You will now learn how to calculate the times each resource has been requested:

    awk '{print $7}' access.log | sort | uniq -c | sort -fr | head
{{< output >}}
 146 /robots.txt
  83 /rss.xml
  56 /
  19 /misc/jquery.js?v=1.4.4
  18 /sites/all/themes/corporateclean/js/jquery.cycle.all.js?prby0d
  18 /sites/all/themes/corporateclean/css/960.css?prby0d
  18 /sites/all/modules/admin_menu/admin_devel/admin_devel.js?prby0d
{{< /output >}}

What you can see here is that strange `prby0d` word, which might be a security
threat coming from outside. It would be good to know whether that request has
been served or not by checking the HTTP code. This can be done as follows:

	grep -w prby0d access.log | awk '{print $9}' | sort | uniq -c | sort -rn
{{< output >}}
 380 200
{{< /output >}}

This means that lines containing the  `prby0d` word are property served all the time.

## Finding Accesses per Hour

This example counts the number of accesses per hour of the day:

    cat access.log | cut -d\[ -f2 | cut -d\] -f1 | awk -F: '{print $2":00"}' | sort | uniq -c | sort -rn
{{< output >}}
 131 18:00
 130 07:00
 114 11:00
 111 10:00
 100 02:00
  93 12:00
  92 21:00
  90 13:00
  89 19:00
  84 08:00
  81 15:00
  81 14:00
  75 16:00
  67 17:00
  46 09:00
  42 22:00
  40 20:00
  39 03:00
  35 06:00
  28 00:00
  24 05:00
  23 01:00
  20 04:00
  18 23:00
{{< /output >}}

The output shows that the busiest hour was `18:00` whereas the least busy hour was `23:00`.

## Finding Accesses per Minute

The presented code finds the number of accesses per specific time, provided
that you have at least 10 accesses (`$1 > 10`):

    cat access.log | cut -d\[ -f2 | cut -d\] -f1 | awk -F: '{print $2":"$3}' | sort -nk1 -nk2 | uniq -c | sort -rn | awk '{ if ($1 > 10) print $0}'
{{< output >}}
  39 21:59
  39 11:49
  39 10:36
  38 16:31
  37 15:46
  37 07:04
  34 07:50
  32 02:37
  32 02:11
  29 19:52
  28 12:02
  28 11:33
  28 10:47
  27 14:34
  27 13:41
  24 17:21
  20 02:03
  17 08:53
  16 21:56
{{< /output >}}

The output shows that the busiest minute was `21:59` whereas the least busy minute
was `21:56`.

## Finding the Total Visits today

You can find the total visits today as follows:

    cat access.log | grep `date '+%e/%b/%G'` | awk '{print $1}' | sort | uniq | wc -l
{{< output >}}
179
{{< /output >}}

Notice that the output of `date '+%e/%b/%G'` is similar to `25/Jun/2019`, which
looks like the way dates appear on Apache log files. The value of `date '+%e/%b/%G'`
will change depending on the current day.

Also have in mind that ``cat access.log | grep `date '+%e/%b/%G'` | awk '{print $1}'``
returns IP addresses that are sorted using `sort(1)`, filtered with `uniq` and
counted using `wc -l`. Therefore each visit is connected to a single IP address.

## Number of unique IPs per day of the month

The following code calculates the number of unique IPs on a per day of the month
basis:

    cat access.log | awk {'print $1, $4'} | awk -F/ {'print $1'} | awk -F\[ {'print $2, $1'} | sort | uniq | awk {'print $1'} | uniq -c | awk {'print $2, $1'}
{{< output >}}
24 147
25 179
{{< /output >}}

Notice that depending on your log data, you might get a different output, which is
the case for all presented examples.

## Bytes served per Day of the Week

The following AWK script, saved as `bytesDOW.awk`, calculates the number of bytes
served by the web server per day of the week:

{{< file "bytesDOW.awk" awk >}}
 # 0 is Sunday, etc.
function dayOfWeek(year, month, day) {
    day_of_week = 0;

    if (month <= 2)
    {
        month += 12;
        year--;
    }

    day_of_week = (day + month * 2 + int(((month + 1) * 6) / 10) + year + int(year / 4) - int(year / 100) + int(year / 400) + 2);
    day_of_week = day_of_week % 7;
    return ((day_of_week ? day_of_week : 7) - 1);
}


BEGIN {
	month["Jan"] = 1;	month["Feb"] = 2;	month["Mar"] = 3;
	month["Apr"] = 4;	month["May"] = 5;	month["Jun"] = 6;
	month["Jul"] = 7;	month["Aug"] = 8;	month["Sep"] = 9;
	month["Oct"] = 10;	month["Nov"] = 11;	month["Dec"] = 12;
}

{
 split($4, left, ":");
 split(left[1], desired, "/");

 year = desired[3];
 myMonth = month[desired[2]];
 day = substr(desired[1], 2);
 currentDay = dayOfWeek(year, myMonth, day);
 myData[currentDay] += $10;

}

END {
	for (val in myData)
		print val,"\t", myData[val];
}
{{< /file >}}

Notice that the 10th field holds the number of bytes transferred.

Executing `bytesDOW.awk` will generate the following kind of output:

    awk -f bytesDOW.awk access.log
{{< output >}}
1 	 9819670
2 	 17575190
{{< /output >}}

As a week has 7 days, the output will have at most 7 lines. Also, have in
mind that 0 is for Sunday, 1 for Monday, etc.

## Connections per Day of the Week

This section will present a variation of the `bytesDOW.awk` script that is saved
as `connectionsDOW.awk` and calculates the number of connections per day of the week:

{{< file "connectionsDOW.awk" awk >}}
 # 0 is Sunday, etc.
function dayOfWeek(year, month, day) {
    day_of_week = 0;

    if (month <= 2)
    {
        month += 12;
        year--;
    }

    day = (day + month * 2 + int(((month + 1) * 6) / 10) + year + int(year / 4) - int(year / 100) + int(year / 400) + 2);
    day_of_week = day % 7;
    return ((day_of_week ? day_of_week : 7) - 1);
}


BEGIN {
	month["Jan"] = 1;	month["Feb"] = 2;	month["Mar"] = 3;
	month["Apr"] = 4;	month["May"] = 5;	month["Jun"] = 6;
	month["Jul"] = 7;	month["Aug"] = 8;	month["Sep"] = 9;
	month["Oct"] = 10;	month["Nov"] = 11;	month["Dec"] = 12;
}

{
 split($4, left, ":");
 split(left[1], desired, "/");

 year = desired[3];
 myMonth = month[desired[2]];
 day = substr(desired[1], 2);
 currentDay = dayOfWeek(year, myMonth, day);
 myData[currentDay]++;
}

END {
	for (val in myData)
		print val,"\t", myData[val];
}
{{< /file >}}

The way AWK finds out the day of the week needs some explanation as it is not a trivial
task because the name of the day cannot be found in the log file and therefore has to be
calculated â€“ the name of the function that does the job is `dayOfWeek()`. The code in
`dayOfWeek()` is based on an algorithm that allows you to know the day of the week for a
date like 25 March 2011. You can learn more about the algorithm that is used in
[here](https://stackoverflow.com/questions/6385190/correctness-of-sakamotos-algorithm-to-find-the-day-of-week/6385934#6385934)
and [here](https://en.wikipedia.org/wiki/Determination_of_the_day_of_the_week).

Executing `connectionsDOW.awk` will generate the following kind of output:

    awk -f connectionsDOW.awk access.log
{{< output >}}
1 	 657
2 	 996
{{< /output >}}

## Summary

AWK is a powerful programming language that can help you process your Apache log files,
extract useful information and generate reports without having to write lots of code.

You can modify the presented examples to match your needs and extend them to create
more amazing things!
