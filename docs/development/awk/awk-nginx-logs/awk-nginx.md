---
author:
  name: Mihalis Tsoukalos
  email: mihalistsoukalos@gmail.com
description: 'AWK for Nginx web server administrators.'
keywords: ["UNIX", "shell", "AWK", "NGINX", "logs"]
license: '[CC BY-ND 4.0](https://creativecommons.org/licenses/by-nd/4.0)'
published: 2019-07-02
modified_by:
  name: Linode
title: 'Using AWK for processing Nginx log files'
contributor:
  name: Mihalis Tsoukalos
  link: https://www.mtsoukalos.eu/
external_resources:
  - '[Nginx](https://nginx.org/)'
  - '[Nginx](https://www.nginx.com/)'
---

## Before You Begin

{{< note >}}
This guide is written for a non-root user. Depending on your configuration, some commands might require the help of `sudo` in order to get property executed. If you are not familiar with the `sudo` command, see the [Users and Groups](/docs/tools-reference/linux-users-and-groups/) guide.
{{< /note >}}

## Introduction

The purpose of this guide is to illustrate how to use the AWK programming language for
processing access log files generated by the Nginx web server.

Most of the presented examples are relatively small and therefore you will not need
to save the AWK code in an external file. However, you are free to do so if you want.

Additionally, most of the commands are using traditional UNIX utilities
such as `grep(1)`, `head(1)`, `sort(1)` and `uniq(1)` to filter or process the
data.

###  An Introduction to Nginx Log Files

The section will talk about the format of Nginx log files. The default format of Nginx
log files looks as follows:

{{< note >}}
54.186.10.255 - - [04/Jun/2015:07:06:05 +0000] "GET /downloads/product_2 HTTP/1.1" 200 2582 "-" "urlgrabber/3.9.1 yum/3.4.3"
80.91.33.133 - - [04/Jun/2015:07:06:16 +0000] "GET /downloads/product_1 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.16)"
{{< /note >}}

This kind of log entries were created using the `combined` log format of Nginx that is
defined as follows:

{{< output >}}
log_format combined '$remote_addr - $remote_user [$time_local]'
        '"$request" $status $body_bytes_sent '
        '"$http_referer" "$http_user_agent"';
{{< /output >}}

Fortunately, the names of the fields are pretty self explanatory and require no further
discussion. The only thing that might be missing from the default format is the
`request_time` field that shows how long it took Nginx to process the request.

You can find more about the fields of the Nginx access log files by visiting
[this](http://nginx.org/en/docs/http/ngx_http_log_module.html).

{{< note >}}
Notice that if the log file format for a Nginx web site is not specified then
the predefined `combined` format is automatically used by the Nginx web server.
{{< /note >}}

The Nginx access log file that will be used for the examples of this guide is taken from
[here](https://github.com/elastic/examples/blob/master/Common%20Data%20Formats/nginx_logs/nginx_logs)
and uses the `combined` log format. However, you are free to use your own Nginx log files.

The output of the `wc(1)` utility will show the number of log entries that can be found in
the used log file:

	wc access.log
{{< output >}}
    51462  721886 6991577 access.log
{{< /output >}}

This means that this is an access log file with **51462** records (*log entries*).

### Changing the format of Nginx Log Files

Nginx allows you change the format of its access log files on a per web site basis,
just like Apache does. For example you can define your own log format named `my_format`
as follows:

{{< output >}}
log_format  my_format  '$remote_addr - $remote_user [$time_local] "$request" '
          '$status $body_bytes_sent "$http_referer" '
          '"$http_user_agent" "$http_x_forwarded_for" "$request_time" ';
{{< /output >}}

Using the `my_format` log format is as simple as adding a line to your Nginx configuration
file that is similar to the following:

    grep my_format /etc/nginx/mtsoukalos.conf
{{< output >}}
access_log  /var/www/logs/access.log  my_format;
{{< /output >}}

## Calculating the Total Number of Bytes Served

The first example will calculate the total number of bytes served:

	cat access.log | awk '{ sum += $10 } END { print sum }'
{{< output >}}
33939678630
{{< /output >}}

In this example, the AWK code adds the values of the 10th field and prints their total.

Please notice that this command can be rewritten as:

    awk '{ sum += $10 } END { print sum }' access.log

This is true for most presented AWK commands.

## Printing the Number of Unique IP addresses

The following command counts the number of unique IP addresses stored on a Nginx log file:

    cat access.log | awk '{print $1}' | sort | uniq | wc -l
{{< output >}}
2660
{{< /output >}}

The `awk '{print $1}'` command prints the first column of each line, which is the IP
address of the client machine. The remaining commands sort the output, filter out
repeated lines in the output and count the number of lines the exist in the generated output.

## Extracting Specific Lines

This technique uses a simple regular expression to extract the lines in the log
file that contain a specific string, which is this case is `/downloads/product_2`:

    awk '/\/downloads\/product_2/' access.log
{{< output >}}
173.255.199.22 - - [04/Jun/2015:07:06:08 +0000] "GET /downloads/product_2 HTTP/1.1" 304 0 "-" "Debian APT-HTTP/1.3 (0.8.10.3)"
141.138.90.60 - - [04/Jun/2015:07:06:46 +0000] "GET /downloads/product_2 HTTP/1.1" 200 3316 "-" "Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.15)"
{{< /output >}}

You are free to continue processing the generated output or search for something different.

Notice that you will need to escape all `/` characters in the URL because regular expressions in AWK
are embedded in `/` characters.

## Finding the HTTP protocol used along with its Version

This example will illustrate how to extract HTTP protocol related information from
your Nginx log files:

    awk '{print $8}' access.log | sort | uniq -c | sort -rn
{{< output >}}
51462 HTTP/1.1"
{{< /output >}}

The output shows that all requests are using the `HTTP/1.1` protocol.

## Finding Information about Status Codes

This is a truly handy case as you will extract information about the status codes
of your requests:

    awk '{print $9}' access.log | sort | uniq -c | sort -rn
{{< output >}}
33876 404
13330 304
4028  200
 186  206
  38  403
   4  416
{{< /output >}}

As the status code is stored in the 9th field of each log record, you will have to
extract it before processing the data using traditional UNIX utilities.

The output shows that there exist too many `404` status codes, which means that the
web server administrator might need to look into it. Generally speaking, a large number
of `404` status codes indicates a server misconfiguration or multiple hacking attempts.

## Creating a Histogram

This section presents a pretty clever use of AWK and its capabilities by printing a
histogram on the screen instead of numeric values.

The following AWK code prints a histogram of the most 10 popular IP addresses that
were found in the current log file:

	cat access.log | awk '{print $1}' | sort | uniq -c | sort -nr | awk '{printf("\n%s ",$0) ; for (i = 0; i<$1/100 ; i++) {printf("*")};}' | head
{{< output >}}
	2350 216.46.173.126 ************************
	1720 180.179.174.219 ******************
	1439 204.77.168.241 ***************
	1365 65.39.197.164 **************
	1202 80.91.33.133 *************
	1120 84.208.15.12 ************
	1084 74.125.60.158 ***********
	1064 119.252.76.162 ***********
	 628 79.136.114.202 *******
{{< /output >}}

The same technique can be used for presenting other kinds of information such as
the HTTP status codes:

    cat access.log | awk '{print $9}' | sort | uniq -c | sort -nr | awk '{printf("\n%s ",$0) ; for (i = 0; i<$1/100 ; i++) {printf("*")};}'
{{< output >}}
33876 404 ***************************************************************************************************************************************************************************************************************************************************************************************************************************************************
13330 304 **************************************************************************************************************************************
4028 200 *****************************************
 186 206 **
  38 403 *
   4 416 *
{{< /output >}}

As we are dealing with a pretty big access log and as certain IP addresses and HTTP status codes appear
too many times, each `*` represented 100 appearances – this is implemented with the `i<$1/100` statement
in the `for` loop. You are free to use any number that fits your needs.

## Extracting all unique user agents

You can extract the user agents that were found in a log file and count the number
of the appearances of each user agent as follows:

    awk -F\" '($2 ~ "^GET /"){print $6}' ./access.log | sort | uniq -c
{{< output >}}
  11 -
   4 Apache-HttpClient/4.3.5 (java 1.5)
   6 Axel 2.4 (Linux)
   5 Chef Client/10.18.2 (ruby-1.8.7-p249; ohai-6.16.0; x86_64-linux; +http://opscode.com)
   ...
{{< /output >}}

If you want to sort the output based on the number of times each user agent was
found in the log files, you can execute the next command:

    awk -F\" '($2 ~ "^GET /"){print $6}' ./access.log | sort | uniq -c | sort -rn | head
{{< output >}}
11830 Debian APT-HTTP/1.3 (1.0.1ubuntu2)
11365 Debian APT-HTTP/1.3 (0.9.7.9)
6719 Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.21)
5740 Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.16)
3855 Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.22)
1827 Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.17)
1255 Debian APT-HTTP/1.3 (0.8.16~exp12ubuntu10.7)
 792 urlgrabber/3.9.1 yum/3.2.29
 750 Debian APT-HTTP/1.3 (0.9.7.8)
 708 urlgrabber/3.9.1 yum/3.4.3
{{< /output >}}

## Calculating the number of times an IP Accessed a URL

This technique calculates the number of times each IP accessed a given
URL (`/downloads/product_2`).

	awk -F'[ "]+' '$7 == "/downloads/product_2" { ipcount[$1]++ }
	    END { for (i in ipcount) {
	        printf "%15s - %d\n", i, ipcount[i] } }' ./access.log
{{< output >}}
  208.65.107.61 - 14
     5.9.143.77 - 16
   78.47.40.226 - 3
   185.39.80.16 - 18
  104.236.43.33 - 1
  23.92.104.200 - 14
   8.28.119.195 - 1
   54.229.83.18 - 1
  87.242.74.154 - 15
 54.221.160.177 - 1
 ...
{{< /output >}}

The `$7` field is the requested URL, which is this case is compared against `/downloads/product_2`.
However, you can use any URL you like or even a regular expression. As we are trying a direct match
using `==`, there is no need to escape the `/` characters of the URL.

If you want, you can sort the generated output numerically:

	awk -F'[ "]+' '$7 == "/downloads/product_2" { ipcount[$1]++ }
	    END { for (i in ipcount) {
	        printf "%15s - %d\n", i, ipcount[i] } }' ./access.log | sort -nr -k 3,3
{{< output >}}
180.179.174.219 - 1720
 204.77.168.241 - 1439
 204.77.169.137 - 448
   46.29.152.10 - 380
 209.216.233.52 - 360
  94.75.199.161 - 359
  ...
{{< /output >}}

The `-k 3,3` option tells `sort(1)` to use the third column when sorting.

## Which URLs have been accessed by a Given IP 

This technique allows you to calculate how many times each URL has been accessed
by a given IP address:

	awk -F'[ "]+' '$1 == "209.216.233.52" { pagecount[$7]++ }
	    END { for (i in pagecount) {
	        printf "%15s - %d\n", i, pagecount[i] } }' access.log
{{< output >}}
/downloads/product_2 - 360
{{< /output >}}

This means that the `209.216.233.52` IP address has only accessed `/downloads/product_2`.

## Calculating the times each Resource has been Requested

You will now learn how to calculate the total times each resource has been requested:

    awk '{print $7}' access.log | sort | uniq -c | sort -fr
{{< output >}}
30285 /downloads/product_1
21104 /downloads/product_2
  73 /downloads/product_3
{{< /output >}}

## Finding Accesses per Hour

This example counts the number of accesses per hour of the day:

    cat access.log | cut -d\[ -f2 | cut -d\] -f1 | awk -F: '{print $2":00"}' | sort | uniq -c | sort -rn
{{< output >}}
2209 20:00
2182 05:00
2178 22:00
2178 15:00
2176 23:00
2174 12:00
2170 13:00
2168 00:00
2166 18:00
2163 01:00
2159 09:00
2157 04:00
2151 14:00
2145 03:00
2135 02:00
2131 10:00
2124 19:00
2120 17:00
2119 16:00
2116 06:00
2106 21:00
2102 11:00
2092 08:00
2041 07:00
{{< /output >}}

The output shows that the busiest hour was `20:00` whereas the least busy hour was `07:00`.

## Finding Accesses per Minute

The presented code finds the number of accesses per specific time, provided
that you have at least 10 accesses (`$1 > 10`):

    cat access.log | cut -d\[ -f2 | cut -d\] -f1 | awk -F: '{print $2":"$3}' | sort -nk1 -nk2 | uniq -c | sort -rn | awk '{ if ($1 > 10) print $0}'
{{< output >}}
1830 22:05
1829 20:05
1823 15:05
1812 13:05
1804 09:05
1803 23:05
1802 12:05
1796 14:05
1790 18:05
1780 19:05
1776 10:05
1772 16:05
1771 17:05
1760 11:05
...
 347 16:06
 344 19:06
 342 11:06
{{< /output >}}

The output shows that the busiest minute was `22:05` whereas the least busy minute
was `11:06`.

## Counting Requests per Second

The following command will count the number of requests per second:

    cat access.log | awk '{print $4}' | uniq -c | sort -rn | head
{{< output >}}
   1 [31/May/2015:23:05:59
   1 [31/May/2015:23:05:58
   1 [31/May/2015:23:05:57
   1 [31/May/2015:23:05:57
   1 [31/May/2015:23:05:57
   ...
{{< /output >}}

## Finding the Total Visits today

You can find the total visits today as follows:

    cat access.log | grep `date '+%e/%b/%G'` | awk '{print $1}' | sort | uniq | wc -l
{{< output >}}
0
{{< /output >}}

Notice that the output of `date '+%e/%b/%G'` is similar to `2/Jul/2019`, which
looks like the way dates appear on Nginx log files. The value of `date '+%e/%b/%G'`
will change depending on the current day.

Also have in mind that ``cat access.log | grep `date '+%e/%b/%G'` | awk '{print $1}'``
returns IP addresses that are sorted using `sort(1)`, filtered with `uniq` and
counted using `wc -l`. Therefore each visit is connected to a single IP address.

## Finding the Total Visits this month

The following code counts the total number of visits up to now for the current month:

    cat access.log | grep `date '+%b/%G'` | awk '{ print $1 }' | sort | uniq -c | wc -l
{{< output >}}
0
{{< /output >}}

Notice that the output of `date '+%b/%G'` is similar to `Jul/2019`, which
looks like the way months appear on Nginx log files. The value of `date '+%b/%G'`
will change depending on the current month.

## Number of unique IPs per day of the month

The following code calculates the number of unique IPs on a per day of the month
basis:

    cat access.log | awk {'print $1, $4'} | awk -F/ {'print $1'} | awk -F\[ {'print $2, $1'} | sort | uniq | awk {'print $1'} | uniq -c | awk {'print $2, $1'}
{{< output >}}
01 238
02 230
03 263
04 101
17 118
18 252
19 276
20 289
21 280
22 254
23 259
24 262
25 283
26 218
27 288
28 239
29 247
30 260
31 267
{{< /output >}}

## Bytes served per Day of the Week

The following AWK script, saved as `bytesDOW.awk`, calculates the number of bytes
served by the Nginx web server per day of the week:

{{< file "bytesDOW.awk" awk >}}
 # 0 is Sunday, etc.
function dayOfWeek(year, month, day) {
    day_of_week = 0;

    if (month <= 2)
    {
        month += 12;
        year--;
    }

    day_of_week = (day + month * 2 + int(((month + 1) * 6) / 10) + year + int(year / 4) - int(year / 100) + int(year / 400) + 2);
    day_of_week = day_of_week % 7;
    return ((day_of_week ? day_of_week : 7) - 1);
}


BEGIN {
	month["Jan"] = 1;	month["Feb"] = 2;	month["Mar"] = 3;
	month["Apr"] = 4;	month["May"] = 5;	month["Jun"] = 6;
	month["Jul"] = 7;	month["Aug"] = 8;	month["Sep"] = 9;
	month["Oct"] = 10;	month["Nov"] = 11;	month["Dec"] = 12;
}

{
 split($4, left, ":");
 split(left[1], desired, "/");

 year = desired[3];
 myMonth = month[desired[2]];
 day = substr(desired[1], 2);
 currentDay = dayOfWeek(year, myMonth, day);
 myData[currentDay] += $10;

}

END {
	for (val in myData)
		print val,"\t", myData[val];
}
{{< /file >}}

Notice that the 10th field holds the number of bytes transferred.

Executing `bytesDOW.awk` will generate the following kind of output:

    awk -f bytesDOW.awk access.log
{{< output >}}
0 	 4121632567
1 	 4742569257
2 	 5188769783
3 	 7285918087
4 	 4730715297
5 	 4316551159
6 	 3553522480
{{< /output >}}

As a week has 7 days, the output will have at most 7 lines. Also, have in
mind that `0` is for Sunday, `1` for Monday, etc. – this is defined in the
AWK code of `bytesDOW.awk`.

## Connections per Day of the Week

This section will present a variation of the `bytesDOW.awk` script that is saved
as `connectionsDOW.awk` and calculates the number of connections per day of the week:

{{< file "connectionsDOW.awk" awk >}}
 # 0 is Sunday, etc.
function dayOfWeek(year, month, day) {
    day_of_week = 0;

    if (month <= 2)
    {
        month += 12;
        year--;
    }

    day = (day + month * 2 + int(((month + 1) * 6) / 10) + year + int(year / 4) - int(year / 100) + int(year / 400) + 2);
    day_of_week = day % 7;
    return ((day_of_week ? day_of_week : 7) - 1);
}


BEGIN {
	month["Jan"] = 1;	month["Feb"] = 2;	month["Mar"] = 3;
	month["Apr"] = 4;	month["May"] = 5;	month["Jun"] = 6;
	month["Jul"] = 7;	month["Aug"] = 8;	month["Sep"] = 9;
	month["Oct"] = 10;	month["Nov"] = 11;	month["Dec"] = 12;
}

{
 split($4, left, ":");
 split(left[1], desired, "/");

 year = desired[3];
 myMonth = month[desired[2]];
 day = substr(desired[1], 2);
 currentDay = dayOfWeek(year, myMonth, day);
 myData[currentDay]++;
}

END {
	for (val in myData)
		print val,"\t", myData[val];
}
{{< /file >}}

The way AWK finds out the day of the week needs some explanation as it is not a trivial
task because the name of the day cannot be found in the log file and therefore has to be
calculated – the name of the function that does the job is `dayOfWeek()`. The code in
`dayOfWeek()` is based on an algorithm that allows you to know the day of the week for a
date like 25 March 2014. You can learn more about the algorithm that is used in
[here](https://stackoverflow.com/questions/6385190/correctness-of-sakamotos-algorithm-to-find-the-day-of-week/6385934#6385934)
and [here](https://en.wikipedia.org/wiki/Determination_of_the_day_of_the_week).

Executing `connectionsDOW.awk` will generate the following kind of output:

    awk -f connectionsDOW.awk access.log
{{< output >}}
0 	 7682
1 	 8531
2 	 8574
3 	 8563
4 	 6629
5 	 5715
6 	 5768
{{< /output >}}

## Summary

AWK is a powerful programming language that can help you process your Nginx log files,
extract useful information and generate reports without having to write lots of code.

You can modify the presented examples to match your needs and extend them to create
amazing things!
